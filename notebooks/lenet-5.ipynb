{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41cb3a05-5709-4f64-9413-cb325cecedc9",
   "metadata": {},
   "source": [
    "# LeNet-5\n",
    "https://www.cnblogs.com/gshang/p/13099170.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddead766-172d-49ef-bd6f-baba1324f2b3",
   "metadata": {},
   "source": [
    "## Load Dependent Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e97b848b-64f4-4bb7-981f-cfbb5af5f3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4d5843-3001-40d3-9388-e1c1a23c3bf8",
   "metadata": {},
   "source": [
    "## Device Configuration: CPU or GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e0eb9df-7f49-4a00-abaf-f18f7d021d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4cfcd6-1baa-4b13-95a4-19b6bafd8d14",
   "metadata": {},
   "source": [
    "## Select Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff377636-fb5a-401c-ac99-d36a59c52959",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b961d9-d4e4-4cc5-8a26-56f9126f770d",
   "metadata": {},
   "source": [
    "## Define MNISTDataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44545a22-3bc2-4d67-898a-63750c535355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import os\n",
    "class MNISTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, train=True, transform=None):\n",
    "        self.file_pre = 'train' if train == True else 't10k'\n",
    "        self.transform = transform\n",
    "        self.label_path = os.path.join(root, '%s-labels-idx1-ubyte.gz' % self.file_pre)\n",
    "        self.image_path = os.path.join(root, '%s-images-idx3-ubyte.gz' % self.file_pre)\n",
    "        self.images, self.labels = self.__read_data__(self.image_path, self.label_path)\n",
    "    \n",
    "    def __read_data__(self, image_path, label_path):\n",
    "        # Read dataset.\n",
    "        with gzip.open(label_path, 'rb') as lbpath:\n",
    "            labels = np.frombuffer(lbpath.read(), np.uint8, offset=8)\n",
    "        with gzip.open(image_path, 'rb') as imgpath:\n",
    "            images = np.frombuffer(imgpath.read(), np.uint8, offset=16).reshape(len(labels), 28, 28)\n",
    "        return images, labels\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.images[index], int(self.labels[index])\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(np.array(image))\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7fbdc5-86f5-406f-87f1-621e79f7088c",
   "metadata": {},
   "source": [
    "## Download through Local Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe9f1c5b-c2d4-414f-b58e-8ef8db7d047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MNISTDataset('../data/MNIST/', transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.1037,), (0.3081,))]))\n",
    "test_dataset = MNISTDataset('../data/MNIST/', train=False, transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.1037,), (0.3081,))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2d0ddf-ad88-443c-bbda-ca69c64d0658",
   "metadata": {},
   "source": [
    "## Set Data Loader (Input Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57b62a6b-3958-470f-83bd-e8b75372b110",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0d6dc8-889b-4998-8e32-de6cc98a9d7a",
   "metadata": {},
   "source": [
    "```python\n",
    "def fc_in(image, Conv, Pool):\n",
    "    for i, j in zip(Conv, Pool):\n",
    "        hk = (image[0] - i[0] + 2 * i[2]) / i[1] + 1\n",
    "        wk = (image[1] - i[0] + 2 * i[2]) / i[1] + 1\n",
    "        hp = (hk - j[0] + 2 * j[2]) / j[1] + 1\n",
    "        wp = (wk - j[0] + 2 * j[2]) / j[1] + 1\n",
    "        image = (hp, wp)\n",
    "    return (int(image[0]), int(image[1]))\n",
    "fc_in((32, 32), ((5, 1, 0), (5, 1, 0)), ((2, 2, 0), (2, 2, 0)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2d5a5e-539e-4ce2-8a14-f2fd0dd34847",
   "metadata": {},
   "source": [
    "## Define LeNet5 Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6acdeff-c9af-4a19-ba97-bf661f863418",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.layer1 = torch.nn.Sequential(torch.nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
    "                                          torch.nn.BatchNorm2d(6),\n",
    "                                          torch.nn.ReLU(),\n",
    "                                          torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = torch.nn.Sequential(torch.nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "                                          torch.nn.BatchNorm2d(16),\n",
    "                                          torch.nn.ReLU(),\n",
    "                                          torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc1 = torch.nn.Sequential(torch.nn.Linear(4 * 4 * 16, 120),\n",
    "                                       torch.nn.ReLU())\n",
    "        self.fc2 = torch.nn.Sequential(torch.nn.Linear(120, 84),\n",
    "                                       torch.nn.ReLU())\n",
    "        self.fc3 = torch.nn.Linear(84, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6778580e-bd4c-4133-b371-947ba376e4f1",
   "metadata": {},
   "source": [
    "## Make Model with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb318f4b-cc02-49f5-838c-aa909fcfd175",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5ad960-025e-4757-b983-89c4b0603e33",
   "metadata": {},
   "source": [
    "## Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3675f68-816e-4e75-b2da-8061faef3e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cec989-9496-477a-a0bc-7adce906b722",
   "metadata": {},
   "source": [
    "## Train the Model with Bacth_Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27366ba9-0be0-44fe-93ce-1c4a94744784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/600], Loss 0.2336\n",
      "Epoch [1/10], Step [200/600], Loss 0.1882\n",
      "Epoch [1/10], Step [300/600], Loss 0.1825\n",
      "Epoch [1/10], Step [400/600], Loss 0.0446\n",
      "Epoch [1/10], Step [500/600], Loss 0.0591\n",
      "Epoch [1/10], Step [600/600], Loss 0.0522\n",
      "Epoch [2/10], Step [100/600], Loss 0.0988\n",
      "Epoch [2/10], Step [200/600], Loss 0.0677\n",
      "Epoch [2/10], Step [300/600], Loss 0.1073\n",
      "Epoch [2/10], Step [400/600], Loss 0.0100\n",
      "Epoch [2/10], Step [500/600], Loss 0.0600\n",
      "Epoch [2/10], Step [600/600], Loss 0.0733\n",
      "Epoch [3/10], Step [100/600], Loss 0.0401\n",
      "Epoch [3/10], Step [200/600], Loss 0.0150\n",
      "Epoch [3/10], Step [300/600], Loss 0.0195\n",
      "Epoch [3/10], Step [400/600], Loss 0.0393\n",
      "Epoch [3/10], Step [500/600], Loss 0.0109\n",
      "Epoch [3/10], Step [600/600], Loss 0.0085\n",
      "Epoch [4/10], Step [100/600], Loss 0.0542\n",
      "Epoch [4/10], Step [200/600], Loss 0.0051\n",
      "Epoch [4/10], Step [300/600], Loss 0.0125\n",
      "Epoch [4/10], Step [400/600], Loss 0.0228\n",
      "Epoch [4/10], Step [500/600], Loss 0.0099\n",
      "Epoch [4/10], Step [600/600], Loss 0.0271\n",
      "Epoch [5/10], Step [100/600], Loss 0.0084\n",
      "Epoch [5/10], Step [200/600], Loss 0.0235\n",
      "Epoch [5/10], Step [300/600], Loss 0.0030\n",
      "Epoch [5/10], Step [400/600], Loss 0.0748\n",
      "Epoch [5/10], Step [500/600], Loss 0.0296\n",
      "Epoch [5/10], Step [600/600], Loss 0.0156\n",
      "Epoch [6/10], Step [100/600], Loss 0.0418\n",
      "Epoch [6/10], Step [200/600], Loss 0.0258\n",
      "Epoch [6/10], Step [300/600], Loss 0.0064\n",
      "Epoch [6/10], Step [400/600], Loss 0.0264\n",
      "Epoch [6/10], Step [500/600], Loss 0.0454\n",
      "Epoch [6/10], Step [600/600], Loss 0.0055\n",
      "Epoch [7/10], Step [100/600], Loss 0.0223\n",
      "Epoch [7/10], Step [200/600], Loss 0.0676\n",
      "Epoch [7/10], Step [300/600], Loss 0.0347\n",
      "Epoch [7/10], Step [400/600], Loss 0.0047\n",
      "Epoch [7/10], Step [500/600], Loss 0.0214\n",
      "Epoch [7/10], Step [600/600], Loss 0.1056\n",
      "Epoch [8/10], Step [100/600], Loss 0.0110\n",
      "Epoch [8/10], Step [200/600], Loss 0.0063\n",
      "Epoch [8/10], Step [300/600], Loss 0.0032\n",
      "Epoch [8/10], Step [400/600], Loss 0.0097\n",
      "Epoch [8/10], Step [500/600], Loss 0.0090\n",
      "Epoch [8/10], Step [600/600], Loss 0.0149\n",
      "Epoch [9/10], Step [100/600], Loss 0.0058\n",
      "Epoch [9/10], Step [200/600], Loss 0.0039\n",
      "Epoch [9/10], Step [300/600], Loss 0.0028\n",
      "Epoch [9/10], Step [400/600], Loss 0.0010\n",
      "Epoch [9/10], Step [500/600], Loss 0.1253\n",
      "Epoch [9/10], Step [600/600], Loss 0.0561\n",
      "Epoch [10/10], Step [100/600], Loss 0.0157\n",
      "Epoch [10/10], Step [200/600], Loss 0.0045\n",
      "Epoch [10/10], Step [300/600], Loss 0.0136\n",
      "Epoch [10/10], Step [400/600], Loss 0.0032\n",
      "Epoch [10/10], Step [500/600], Loss 0.0126\n",
      "Epoch [10/10], Step [600/600], Loss 0.0156\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optim\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item())) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db55cc1-fc7f-4dc0-a76d-027d92fe965a",
   "metadata": {},
   "source": [
    "## Test the Model with .eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "969e6db6-83bb-4864-8420-7e9adf269a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 99.09 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print ('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d520860-6b20-468b-b5ea-cd6a315536b3",
   "metadata": {},
   "source": [
    "## Save the Model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d212c73f-6642-4b5d-a67a-7ebe21948f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'LeNet5.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34c5e45-36bb-46a1-9579-c31063fd5c36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
