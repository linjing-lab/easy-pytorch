{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a8dc16d-b947-422c-8b8c-7fdf7f5cb349",
   "metadata": {},
   "source": [
    "# AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc377319-e26d-4322-9d19-4fd48813e974",
   "metadata": {},
   "source": [
    "## Load Dependent Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af80e16a-99a0-481e-b1b4-782acf65eadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc0a2da-0bf0-42ba-a623-9565697fae10",
   "metadata": {},
   "source": [
    "## Device Configuration: CPU or GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f75e41b-5261-429d-9fb8-962c0a915d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d437a7-385e-456a-81dc-5573b3938d5d",
   "metadata": {},
   "source": [
    "## Select Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73097a29-a26c-4f8c-8ed5-f674ba8c1963",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 80\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.0006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befae842-4636-42b1-8c4e-10fcfa1aa567",
   "metadata": {},
   "source": [
    "数据增强：https://blog.csdn.net/weixin_40793406/article/details/84867143"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96606895-c131-4fc8-a1cf-4755ec2b761c",
   "metadata": {},
   "source": [
    "## Transform Configuration and Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9326b6a7-ce39-433b-89b3-870a8ab95a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = torchvision.transforms.Compose([torchvision.transforms.Pad(2),\n",
    "                                                  torchvision.transforms.RandomHorizontalFlip(),\n",
    "                                                  torchvision.transforms.RandomCrop(32),\n",
    "                                                  torchvision.transforms.ToTensor(),\n",
    "                                                  torchvision.transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "\n",
    "transform_test = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                                 torchvision.transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73686cda-69e9-4251-900d-1da75cef326b",
   "metadata": {},
   "source": [
    "## Load Downloaded Dataset (download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "562b3cc2-7ac3-4e70-a07e-31e1e9a8dab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.CIFAR10('../data/CIFAR10/', train=True, download=False, transform=transform_train)\n",
    "test_dataset = torchvision.datasets.CIFAR10('../data/CIFAR10/', train=False, download=False, transform=transform_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213be74e-2497-4ce2-89fd-d90f94ca0714",
   "metadata": {},
   "source": [
    "## Set Data Loader (Input Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "611f0a26-aa6c-40aa-a4d1-abdcadb24a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a4f1250-f3c0-44d9-8575-96a28939098f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(6 - 2 + 2 * 0) / 2 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb2aa5f-f965-490b-b2b1-bac1655ac9d4",
   "metadata": {},
   "source": [
    "## Define AlexNet Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0a650f5-9f3b-47d9-8a23-801b70fe8b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(torch.nn.Module):\n",
    "    def __init__(self, num_classes, init_weights=False):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.layer1 = torch.nn.Sequential(torch.nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=2),\n",
    "                                          torch.nn.BatchNorm2d(64),\n",
    "                                          torch.nn.ReLU(inplace=True),\n",
    "                                          torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=0))\n",
    "        \n",
    "        self.layer2 = torch.nn.Sequential(torch.nn.Conv2d(64, 192, kernel_size=4, stride=1, padding=1),\n",
    "                                          torch.nn.BatchNorm2d(192),\n",
    "                                          torch.nn.ReLU(inplace=True),\n",
    "                                          torch.nn.MaxPool2d(kernel_size=2, stride=1, padding=0))\n",
    "        \n",
    "        self.layer3 = torch.nn.Sequential(torch.nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "                                          torch.nn.BatchNorm2d(384),\n",
    "                                          torch.nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.layer4 = torch.nn.Sequential(torch.nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "                                          torch.nn.BatchNorm2d(256),\n",
    "                                          torch.nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.layer5 = torch.nn.Sequential(torch.nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "                                          torch.nn.BatchNorm2d(256),\n",
    "                                          torch.nn.ReLU(inplace=True),\n",
    "                                          torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.avgpool = torch.nn.Sequential(torch.nn.AdaptiveAvgPool2d(output_size=(3, 3)))\n",
    "        \n",
    "        self.fc1 = torch.nn.Sequential(torch.nn.Dropout(p=0.5, inplace=False),\n",
    "                                       torch.nn.Linear(256 * 3 * 3, 1024),\n",
    "                                       torch.nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.fc2 = torch.nn.Sequential(torch.nn.Dropout(p=0.5, inplace=False),\n",
    "                                       torch.nn.Linear(1024, 1024),\n",
    "                                       torch.nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.fc3 = torch.nn.Sequential(torch.nn.Dropout(p=0.5, inplace=False),\n",
    "                                       torch.nn.Linear(1024, num_classes))\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, torch.nn.Linear):\n",
    "                torch.nn.init.normal_(m.weight, 0, 0.01)\n",
    "                torch.nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773b3c49-f027-4770-99c0-240f26a234b7",
   "metadata": {},
   "source": [
    "## Make Model with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16a8c684-265b-4562-b568-8783f11a0ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlexNet(num_classes, True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4226a6-7254-4243-9578-27ca9c09d267",
   "metadata": {},
   "source": [
    "## Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c31f213-8c92-4c24-a73b-523f16c1f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9fdc64-1702-426d-8e11-daeafb738347",
   "metadata": {},
   "source": [
    "### Update Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b23b9c09-8eec-4a38-a36f-d2a5e5a9d868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a33191c-ea17-4f0f-86bf-595d1476bafe",
   "metadata": {},
   "source": [
    "## Train Model with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb0d2338-7baf-41ae-a51e-6416b93aa8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80], Step [100/500], Loss 1.7641\n",
      "Epoch [1/80], Step [200/500], Loss 1.3219\n",
      "Epoch [1/80], Step [300/500], Loss 1.3826\n",
      "Epoch [1/80], Step [400/500], Loss 1.2343\n",
      "Epoch [1/80], Step [500/500], Loss 1.2426\n",
      "Epoch [2/80], Step [100/500], Loss 1.1751\n",
      "Epoch [2/80], Step [200/500], Loss 1.0635\n",
      "Epoch [2/80], Step [300/500], Loss 1.2144\n",
      "Epoch [2/80], Step [400/500], Loss 1.1216\n",
      "Epoch [2/80], Step [500/500], Loss 1.2333\n",
      "Epoch [3/80], Step [100/500], Loss 0.9114\n",
      "Epoch [3/80], Step [200/500], Loss 1.1976\n",
      "Epoch [3/80], Step [300/500], Loss 1.1030\n",
      "Epoch [3/80], Step [400/500], Loss 0.9300\n",
      "Epoch [3/80], Step [500/500], Loss 0.8047\n",
      "Epoch [4/80], Step [100/500], Loss 1.0986\n",
      "Epoch [4/80], Step [200/500], Loss 0.9220\n",
      "Epoch [4/80], Step [300/500], Loss 1.0176\n",
      "Epoch [4/80], Step [400/500], Loss 0.8595\n",
      "Epoch [4/80], Step [500/500], Loss 0.8597\n",
      "Epoch [5/80], Step [100/500], Loss 0.8618\n",
      "Epoch [5/80], Step [200/500], Loss 0.7139\n",
      "Epoch [5/80], Step [300/500], Loss 0.7958\n",
      "Epoch [5/80], Step [400/500], Loss 0.7768\n",
      "Epoch [5/80], Step [500/500], Loss 0.7691\n",
      "Epoch [6/80], Step [100/500], Loss 0.5139\n",
      "Epoch [6/80], Step [200/500], Loss 0.6152\n",
      "Epoch [6/80], Step [300/500], Loss 1.0465\n",
      "Epoch [6/80], Step [400/500], Loss 1.0016\n",
      "Epoch [6/80], Step [500/500], Loss 0.6458\n",
      "Epoch [7/80], Step [100/500], Loss 0.8082\n",
      "Epoch [7/80], Step [200/500], Loss 0.5068\n",
      "Epoch [7/80], Step [300/500], Loss 0.7627\n",
      "Epoch [7/80], Step [400/500], Loss 0.5556\n",
      "Epoch [7/80], Step [500/500], Loss 0.7198\n",
      "Epoch [8/80], Step [100/500], Loss 0.7202\n",
      "Epoch [8/80], Step [200/500], Loss 0.6470\n",
      "Epoch [8/80], Step [300/500], Loss 0.6386\n",
      "Epoch [8/80], Step [400/500], Loss 0.7186\n",
      "Epoch [8/80], Step [500/500], Loss 0.6135\n",
      "Epoch [9/80], Step [100/500], Loss 0.6240\n",
      "Epoch [9/80], Step [200/500], Loss 0.5691\n",
      "Epoch [9/80], Step [300/500], Loss 0.7103\n",
      "Epoch [9/80], Step [400/500], Loss 0.6406\n",
      "Epoch [9/80], Step [500/500], Loss 0.6084\n",
      "Epoch [10/80], Step [100/500], Loss 0.6846\n",
      "Epoch [10/80], Step [200/500], Loss 0.7685\n",
      "Epoch [10/80], Step [300/500], Loss 0.5787\n",
      "Epoch [10/80], Step [400/500], Loss 0.6777\n",
      "Epoch [10/80], Step [500/500], Loss 0.7976\n",
      "Epoch [11/80], Step [100/500], Loss 0.5981\n",
      "Epoch [11/80], Step [200/500], Loss 0.6369\n",
      "Epoch [11/80], Step [300/500], Loss 0.6897\n",
      "Epoch [11/80], Step [400/500], Loss 0.7244\n",
      "Epoch [11/80], Step [500/500], Loss 0.7192\n",
      "Epoch [12/80], Step [100/500], Loss 0.6136\n",
      "Epoch [12/80], Step [200/500], Loss 0.4713\n",
      "Epoch [12/80], Step [300/500], Loss 0.5265\n",
      "Epoch [12/80], Step [400/500], Loss 0.3413\n",
      "Epoch [12/80], Step [500/500], Loss 0.6516\n",
      "Epoch [13/80], Step [100/500], Loss 0.3706\n",
      "Epoch [13/80], Step [200/500], Loss 0.5273\n",
      "Epoch [13/80], Step [300/500], Loss 0.4635\n",
      "Epoch [13/80], Step [400/500], Loss 0.4092\n",
      "Epoch [13/80], Step [500/500], Loss 0.5533\n",
      "Epoch [14/80], Step [100/500], Loss 0.4799\n",
      "Epoch [14/80], Step [200/500], Loss 0.6048\n",
      "Epoch [14/80], Step [300/500], Loss 0.6792\n",
      "Epoch [14/80], Step [400/500], Loss 0.5487\n",
      "Epoch [14/80], Step [500/500], Loss 0.4629\n",
      "Epoch [15/80], Step [100/500], Loss 0.6445\n",
      "Epoch [15/80], Step [200/500], Loss 0.5277\n",
      "Epoch [15/80], Step [300/500], Loss 0.5890\n",
      "Epoch [15/80], Step [400/500], Loss 0.3530\n",
      "Epoch [15/80], Step [500/500], Loss 0.5788\n",
      "Epoch [16/80], Step [100/500], Loss 0.4965\n",
      "Epoch [16/80], Step [200/500], Loss 0.2906\n",
      "Epoch [16/80], Step [300/500], Loss 0.6767\n",
      "Epoch [16/80], Step [400/500], Loss 0.5363\n",
      "Epoch [16/80], Step [500/500], Loss 0.4706\n",
      "Epoch [17/80], Step [100/500], Loss 0.5821\n",
      "Epoch [17/80], Step [200/500], Loss 0.5379\n",
      "Epoch [17/80], Step [300/500], Loss 0.4441\n",
      "Epoch [17/80], Step [400/500], Loss 0.5429\n",
      "Epoch [17/80], Step [500/500], Loss 0.5264\n",
      "Epoch [18/80], Step [100/500], Loss 0.4435\n",
      "Epoch [18/80], Step [200/500], Loss 0.5012\n",
      "Epoch [18/80], Step [300/500], Loss 0.5045\n",
      "Epoch [18/80], Step [400/500], Loss 0.4144\n",
      "Epoch [18/80], Step [500/500], Loss 0.5710\n",
      "Epoch [19/80], Step [100/500], Loss 0.4274\n",
      "Epoch [19/80], Step [200/500], Loss 0.2969\n",
      "Epoch [19/80], Step [300/500], Loss 0.4994\n",
      "Epoch [19/80], Step [400/500], Loss 0.4280\n",
      "Epoch [19/80], Step [500/500], Loss 0.4401\n",
      "Epoch [20/80], Step [100/500], Loss 0.3992\n",
      "Epoch [20/80], Step [200/500], Loss 0.4964\n",
      "Epoch [20/80], Step [300/500], Loss 0.3566\n",
      "Epoch [20/80], Step [400/500], Loss 0.3521\n",
      "Epoch [20/80], Step [500/500], Loss 0.3508\n",
      "Epoch [21/80], Step [100/500], Loss 0.2900\n",
      "Epoch [21/80], Step [200/500], Loss 0.2297\n",
      "Epoch [21/80], Step [300/500], Loss 0.2261\n",
      "Epoch [21/80], Step [400/500], Loss 0.4758\n",
      "Epoch [21/80], Step [500/500], Loss 0.4890\n",
      "Epoch [22/80], Step [100/500], Loss 0.3667\n",
      "Epoch [22/80], Step [200/500], Loss 0.3665\n",
      "Epoch [22/80], Step [300/500], Loss 0.1952\n",
      "Epoch [22/80], Step [400/500], Loss 0.2012\n",
      "Epoch [22/80], Step [500/500], Loss 0.3427\n",
      "Epoch [23/80], Step [100/500], Loss 0.1604\n",
      "Epoch [23/80], Step [200/500], Loss 0.2385\n",
      "Epoch [23/80], Step [300/500], Loss 0.2243\n",
      "Epoch [23/80], Step [400/500], Loss 0.3277\n",
      "Epoch [23/80], Step [500/500], Loss 0.5465\n",
      "Epoch [24/80], Step [100/500], Loss 0.2575\n",
      "Epoch [24/80], Step [200/500], Loss 0.2778\n",
      "Epoch [24/80], Step [300/500], Loss 0.3190\n",
      "Epoch [24/80], Step [400/500], Loss 0.2286\n",
      "Epoch [24/80], Step [500/500], Loss 0.2783\n",
      "Epoch [25/80], Step [100/500], Loss 0.2768\n",
      "Epoch [25/80], Step [200/500], Loss 0.1820\n",
      "Epoch [25/80], Step [300/500], Loss 0.2143\n",
      "Epoch [25/80], Step [400/500], Loss 0.2597\n",
      "Epoch [25/80], Step [500/500], Loss 0.2246\n",
      "Epoch [26/80], Step [100/500], Loss 0.2879\n",
      "Epoch [26/80], Step [200/500], Loss 0.3605\n",
      "Epoch [26/80], Step [300/500], Loss 0.1744\n",
      "Epoch [26/80], Step [400/500], Loss 0.2280\n",
      "Epoch [26/80], Step [500/500], Loss 0.2902\n",
      "Epoch [27/80], Step [100/500], Loss 0.2109\n",
      "Epoch [27/80], Step [200/500], Loss 0.2089\n",
      "Epoch [27/80], Step [300/500], Loss 0.2052\n",
      "Epoch [27/80], Step [400/500], Loss 0.2671\n",
      "Epoch [27/80], Step [500/500], Loss 0.2367\n",
      "Epoch [28/80], Step [100/500], Loss 0.2102\n",
      "Epoch [28/80], Step [200/500], Loss 0.3337\n",
      "Epoch [28/80], Step [300/500], Loss 0.3264\n",
      "Epoch [28/80], Step [400/500], Loss 0.2459\n",
      "Epoch [28/80], Step [500/500], Loss 0.3107\n",
      "Epoch [29/80], Step [100/500], Loss 0.1964\n",
      "Epoch [29/80], Step [200/500], Loss 0.3009\n",
      "Epoch [29/80], Step [300/500], Loss 0.3481\n",
      "Epoch [29/80], Step [400/500], Loss 0.3294\n",
      "Epoch [29/80], Step [500/500], Loss 0.2124\n",
      "Epoch [30/80], Step [100/500], Loss 0.2230\n",
      "Epoch [30/80], Step [200/500], Loss 0.1895\n",
      "Epoch [30/80], Step [300/500], Loss 0.1744\n",
      "Epoch [30/80], Step [400/500], Loss 0.2507\n",
      "Epoch [30/80], Step [500/500], Loss 0.1796\n",
      "Epoch [31/80], Step [100/500], Loss 0.1377\n",
      "Epoch [31/80], Step [200/500], Loss 0.1530\n",
      "Epoch [31/80], Step [300/500], Loss 0.2242\n",
      "Epoch [31/80], Step [400/500], Loss 0.1136\n",
      "Epoch [31/80], Step [500/500], Loss 0.1977\n",
      "Epoch [32/80], Step [100/500], Loss 0.1816\n",
      "Epoch [32/80], Step [200/500], Loss 0.3895\n",
      "Epoch [32/80], Step [300/500], Loss 0.2034\n",
      "Epoch [32/80], Step [400/500], Loss 0.1169\n",
      "Epoch [32/80], Step [500/500], Loss 0.1928\n",
      "Epoch [33/80], Step [100/500], Loss 0.1685\n",
      "Epoch [33/80], Step [200/500], Loss 0.3069\n",
      "Epoch [33/80], Step [300/500], Loss 0.1529\n",
      "Epoch [33/80], Step [400/500], Loss 0.1540\n",
      "Epoch [33/80], Step [500/500], Loss 0.2725\n",
      "Epoch [34/80], Step [100/500], Loss 0.1702\n",
      "Epoch [34/80], Step [200/500], Loss 0.1696\n",
      "Epoch [34/80], Step [300/500], Loss 0.1808\n",
      "Epoch [34/80], Step [400/500], Loss 0.2559\n",
      "Epoch [34/80], Step [500/500], Loss 0.2116\n",
      "Epoch [35/80], Step [100/500], Loss 0.2322\n",
      "Epoch [35/80], Step [200/500], Loss 0.2389\n",
      "Epoch [35/80], Step [300/500], Loss 0.1423\n",
      "Epoch [35/80], Step [400/500], Loss 0.1183\n",
      "Epoch [35/80], Step [500/500], Loss 0.1800\n",
      "Epoch [36/80], Step [100/500], Loss 0.1191\n",
      "Epoch [36/80], Step [200/500], Loss 0.1688\n",
      "Epoch [36/80], Step [300/500], Loss 0.1953\n",
      "Epoch [36/80], Step [400/500], Loss 0.0765\n",
      "Epoch [36/80], Step [500/500], Loss 0.2485\n",
      "Epoch [37/80], Step [100/500], Loss 0.1448\n",
      "Epoch [37/80], Step [200/500], Loss 0.0936\n",
      "Epoch [37/80], Step [300/500], Loss 0.0773\n",
      "Epoch [37/80], Step [400/500], Loss 0.2353\n",
      "Epoch [37/80], Step [500/500], Loss 0.1625\n",
      "Epoch [38/80], Step [100/500], Loss 0.1805\n",
      "Epoch [38/80], Step [200/500], Loss 0.0963\n",
      "Epoch [38/80], Step [300/500], Loss 0.2301\n",
      "Epoch [38/80], Step [400/500], Loss 0.0824\n",
      "Epoch [38/80], Step [500/500], Loss 0.1107\n",
      "Epoch [39/80], Step [100/500], Loss 0.1141\n",
      "Epoch [39/80], Step [200/500], Loss 0.1720\n",
      "Epoch [39/80], Step [300/500], Loss 0.1341\n",
      "Epoch [39/80], Step [400/500], Loss 0.2794\n",
      "Epoch [39/80], Step [500/500], Loss 0.1653\n",
      "Epoch [40/80], Step [100/500], Loss 0.1788\n",
      "Epoch [40/80], Step [200/500], Loss 0.1926\n",
      "Epoch [40/80], Step [300/500], Loss 0.2253\n",
      "Epoch [40/80], Step [400/500], Loss 0.0890\n",
      "Epoch [40/80], Step [500/500], Loss 0.1187\n",
      "Epoch [41/80], Step [100/500], Loss 0.2145\n",
      "Epoch [41/80], Step [200/500], Loss 0.0807\n",
      "Epoch [41/80], Step [300/500], Loss 0.1993\n",
      "Epoch [41/80], Step [400/500], Loss 0.0959\n",
      "Epoch [41/80], Step [500/500], Loss 0.1962\n",
      "Epoch [42/80], Step [100/500], Loss 0.1283\n",
      "Epoch [42/80], Step [200/500], Loss 0.0960\n",
      "Epoch [42/80], Step [300/500], Loss 0.1217\n",
      "Epoch [42/80], Step [400/500], Loss 0.1775\n",
      "Epoch [42/80], Step [500/500], Loss 0.1270\n",
      "Epoch [43/80], Step [100/500], Loss 0.1427\n",
      "Epoch [43/80], Step [200/500], Loss 0.0190\n",
      "Epoch [43/80], Step [300/500], Loss 0.1162\n",
      "Epoch [43/80], Step [400/500], Loss 0.1564\n",
      "Epoch [43/80], Step [500/500], Loss 0.1741\n",
      "Epoch [44/80], Step [100/500], Loss 0.2085\n",
      "Epoch [44/80], Step [200/500], Loss 0.0931\n",
      "Epoch [44/80], Step [300/500], Loss 0.0802\n",
      "Epoch [44/80], Step [400/500], Loss 0.0640\n",
      "Epoch [44/80], Step [500/500], Loss 0.1550\n",
      "Epoch [45/80], Step [100/500], Loss 0.1403\n",
      "Epoch [45/80], Step [200/500], Loss 0.0939\n",
      "Epoch [45/80], Step [300/500], Loss 0.0478\n",
      "Epoch [45/80], Step [400/500], Loss 0.1037\n",
      "Epoch [45/80], Step [500/500], Loss 0.0558\n",
      "Epoch [46/80], Step [100/500], Loss 0.1440\n",
      "Epoch [46/80], Step [200/500], Loss 0.1264\n",
      "Epoch [46/80], Step [300/500], Loss 0.0482\n",
      "Epoch [46/80], Step [400/500], Loss 0.1401\n",
      "Epoch [46/80], Step [500/500], Loss 0.1251\n",
      "Epoch [47/80], Step [100/500], Loss 0.0811\n",
      "Epoch [47/80], Step [200/500], Loss 0.0604\n",
      "Epoch [47/80], Step [300/500], Loss 0.2298\n",
      "Epoch [47/80], Step [400/500], Loss 0.0614\n",
      "Epoch [47/80], Step [500/500], Loss 0.0898\n",
      "Epoch [48/80], Step [100/500], Loss 0.0989\n",
      "Epoch [48/80], Step [200/500], Loss 0.1796\n",
      "Epoch [48/80], Step [300/500], Loss 0.1284\n",
      "Epoch [48/80], Step [400/500], Loss 0.0298\n",
      "Epoch [48/80], Step [500/500], Loss 0.1037\n",
      "Epoch [49/80], Step [100/500], Loss 0.1131\n",
      "Epoch [49/80], Step [200/500], Loss 0.1701\n",
      "Epoch [49/80], Step [300/500], Loss 0.1576\n",
      "Epoch [49/80], Step [400/500], Loss 0.1022\n",
      "Epoch [49/80], Step [500/500], Loss 0.0624\n",
      "Epoch [50/80], Step [100/500], Loss 0.0793\n",
      "Epoch [50/80], Step [200/500], Loss 0.1453\n",
      "Epoch [50/80], Step [300/500], Loss 0.1071\n",
      "Epoch [50/80], Step [400/500], Loss 0.0707\n",
      "Epoch [50/80], Step [500/500], Loss 0.0749\n",
      "Epoch [51/80], Step [100/500], Loss 0.0336\n",
      "Epoch [51/80], Step [200/500], Loss 0.0464\n",
      "Epoch [51/80], Step [300/500], Loss 0.1640\n",
      "Epoch [51/80], Step [400/500], Loss 0.1063\n",
      "Epoch [51/80], Step [500/500], Loss 0.1276\n",
      "Epoch [52/80], Step [100/500], Loss 0.0706\n",
      "Epoch [52/80], Step [200/500], Loss 0.0522\n",
      "Epoch [52/80], Step [300/500], Loss 0.0688\n",
      "Epoch [52/80], Step [400/500], Loss 0.1412\n",
      "Epoch [52/80], Step [500/500], Loss 0.0815\n",
      "Epoch [53/80], Step [100/500], Loss 0.0580\n",
      "Epoch [53/80], Step [200/500], Loss 0.1588\n",
      "Epoch [53/80], Step [300/500], Loss 0.1738\n",
      "Epoch [53/80], Step [400/500], Loss 0.1022\n",
      "Epoch [53/80], Step [500/500], Loss 0.1263\n",
      "Epoch [54/80], Step [100/500], Loss 0.0617\n",
      "Epoch [54/80], Step [200/500], Loss 0.0733\n",
      "Epoch [54/80], Step [300/500], Loss 0.0652\n",
      "Epoch [54/80], Step [400/500], Loss 0.0355\n",
      "Epoch [54/80], Step [500/500], Loss 0.0675\n",
      "Epoch [55/80], Step [100/500], Loss 0.0716\n",
      "Epoch [55/80], Step [200/500], Loss 0.0846\n",
      "Epoch [55/80], Step [300/500], Loss 0.0511\n",
      "Epoch [55/80], Step [400/500], Loss 0.0417\n",
      "Epoch [55/80], Step [500/500], Loss 0.0446\n",
      "Epoch [56/80], Step [100/500], Loss 0.0520\n",
      "Epoch [56/80], Step [200/500], Loss 0.0704\n",
      "Epoch [56/80], Step [300/500], Loss 0.1479\n",
      "Epoch [56/80], Step [400/500], Loss 0.0653\n",
      "Epoch [56/80], Step [500/500], Loss 0.0918\n",
      "Epoch [57/80], Step [100/500], Loss 0.0477\n",
      "Epoch [57/80], Step [200/500], Loss 0.0821\n",
      "Epoch [57/80], Step [300/500], Loss 0.1887\n",
      "Epoch [57/80], Step [400/500], Loss 0.1413\n",
      "Epoch [57/80], Step [500/500], Loss 0.0597\n",
      "Epoch [58/80], Step [100/500], Loss 0.0713\n",
      "Epoch [58/80], Step [200/500], Loss 0.0273\n",
      "Epoch [58/80], Step [300/500], Loss 0.1529\n",
      "Epoch [58/80], Step [400/500], Loss 0.0766\n",
      "Epoch [58/80], Step [500/500], Loss 0.0131\n",
      "Epoch [59/80], Step [100/500], Loss 0.0767\n",
      "Epoch [59/80], Step [200/500], Loss 0.0787\n",
      "Epoch [59/80], Step [300/500], Loss 0.0798\n",
      "Epoch [59/80], Step [400/500], Loss 0.0376\n",
      "Epoch [59/80], Step [500/500], Loss 0.0897\n",
      "Epoch [60/80], Step [100/500], Loss 0.0426\n",
      "Epoch [60/80], Step [200/500], Loss 0.0500\n",
      "Epoch [60/80], Step [300/500], Loss 0.0450\n",
      "Epoch [60/80], Step [400/500], Loss 0.0585\n",
      "Epoch [60/80], Step [500/500], Loss 0.0860\n",
      "Epoch [61/80], Step [100/500], Loss 0.0563\n",
      "Epoch [61/80], Step [200/500], Loss 0.0481\n",
      "Epoch [61/80], Step [300/500], Loss 0.0860\n",
      "Epoch [61/80], Step [400/500], Loss 0.0590\n",
      "Epoch [61/80], Step [500/500], Loss 0.0957\n",
      "Epoch [62/80], Step [100/500], Loss 0.0103\n",
      "Epoch [62/80], Step [200/500], Loss 0.1143\n",
      "Epoch [62/80], Step [300/500], Loss 0.0585\n",
      "Epoch [62/80], Step [400/500], Loss 0.0348\n",
      "Epoch [62/80], Step [500/500], Loss 0.1085\n",
      "Epoch [63/80], Step [100/500], Loss 0.1178\n",
      "Epoch [63/80], Step [200/500], Loss 0.0377\n",
      "Epoch [63/80], Step [300/500], Loss 0.0835\n",
      "Epoch [63/80], Step [400/500], Loss 0.0365\n",
      "Epoch [63/80], Step [500/500], Loss 0.0601\n",
      "Epoch [64/80], Step [100/500], Loss 0.0952\n",
      "Epoch [64/80], Step [200/500], Loss 0.0589\n",
      "Epoch [64/80], Step [300/500], Loss 0.1490\n",
      "Epoch [64/80], Step [400/500], Loss 0.1010\n",
      "Epoch [64/80], Step [500/500], Loss 0.0857\n",
      "Epoch [65/80], Step [100/500], Loss 0.0140\n",
      "Epoch [65/80], Step [200/500], Loss 0.1076\n",
      "Epoch [65/80], Step [300/500], Loss 0.0636\n",
      "Epoch [65/80], Step [400/500], Loss 0.0659\n",
      "Epoch [65/80], Step [500/500], Loss 0.0342\n",
      "Epoch [66/80], Step [100/500], Loss 0.0355\n",
      "Epoch [66/80], Step [200/500], Loss 0.0273\n",
      "Epoch [66/80], Step [300/500], Loss 0.0562\n",
      "Epoch [66/80], Step [400/500], Loss 0.0631\n",
      "Epoch [66/80], Step [500/500], Loss 0.1566\n",
      "Epoch [67/80], Step [100/500], Loss 0.0600\n",
      "Epoch [67/80], Step [200/500], Loss 0.1364\n",
      "Epoch [67/80], Step [300/500], Loss 0.0340\n",
      "Epoch [67/80], Step [400/500], Loss 0.1170\n",
      "Epoch [67/80], Step [500/500], Loss 0.0268\n",
      "Epoch [68/80], Step [100/500], Loss 0.1350\n",
      "Epoch [68/80], Step [200/500], Loss 0.1076\n",
      "Epoch [68/80], Step [300/500], Loss 0.0617\n",
      "Epoch [68/80], Step [400/500], Loss 0.0293\n",
      "Epoch [68/80], Step [500/500], Loss 0.0340\n",
      "Epoch [69/80], Step [100/500], Loss 0.0339\n",
      "Epoch [69/80], Step [200/500], Loss 0.0366\n",
      "Epoch [69/80], Step [300/500], Loss 0.0693\n",
      "Epoch [69/80], Step [400/500], Loss 0.0457\n",
      "Epoch [69/80], Step [500/500], Loss 0.0270\n",
      "Epoch [70/80], Step [100/500], Loss 0.0302\n",
      "Epoch [70/80], Step [200/500], Loss 0.1406\n",
      "Epoch [70/80], Step [300/500], Loss 0.0695\n",
      "Epoch [70/80], Step [400/500], Loss 0.0204\n",
      "Epoch [70/80], Step [500/500], Loss 0.0250\n",
      "Epoch [71/80], Step [100/500], Loss 0.0521\n",
      "Epoch [71/80], Step [200/500], Loss 0.0534\n",
      "Epoch [71/80], Step [300/500], Loss 0.0278\n",
      "Epoch [71/80], Step [400/500], Loss 0.0205\n",
      "Epoch [71/80], Step [500/500], Loss 0.1292\n",
      "Epoch [72/80], Step [100/500], Loss 0.1853\n",
      "Epoch [72/80], Step [200/500], Loss 0.0419\n",
      "Epoch [72/80], Step [300/500], Loss 0.0536\n",
      "Epoch [72/80], Step [400/500], Loss 0.0486\n",
      "Epoch [72/80], Step [500/500], Loss 0.0789\n",
      "Epoch [73/80], Step [100/500], Loss 0.0555\n",
      "Epoch [73/80], Step [200/500], Loss 0.0096\n",
      "Epoch [73/80], Step [300/500], Loss 0.0772\n",
      "Epoch [73/80], Step [400/500], Loss 0.1175\n",
      "Epoch [73/80], Step [500/500], Loss 0.0343\n",
      "Epoch [74/80], Step [100/500], Loss 0.0465\n",
      "Epoch [74/80], Step [200/500], Loss 0.0292\n",
      "Epoch [74/80], Step [300/500], Loss 0.0338\n",
      "Epoch [74/80], Step [400/500], Loss 0.0265\n",
      "Epoch [74/80], Step [500/500], Loss 0.0211\n",
      "Epoch [75/80], Step [100/500], Loss 0.0392\n",
      "Epoch [75/80], Step [200/500], Loss 0.0449\n",
      "Epoch [75/80], Step [300/500], Loss 0.0467\n",
      "Epoch [75/80], Step [400/500], Loss 0.0776\n",
      "Epoch [75/80], Step [500/500], Loss 0.0498\n",
      "Epoch [76/80], Step [100/500], Loss 0.0012\n",
      "Epoch [76/80], Step [200/500], Loss 0.1425\n",
      "Epoch [76/80], Step [300/500], Loss 0.0285\n",
      "Epoch [76/80], Step [400/500], Loss 0.0451\n",
      "Epoch [76/80], Step [500/500], Loss 0.0855\n",
      "Epoch [77/80], Step [100/500], Loss 0.1213\n",
      "Epoch [77/80], Step [200/500], Loss 0.0198\n",
      "Epoch [77/80], Step [300/500], Loss 0.0617\n",
      "Epoch [77/80], Step [400/500], Loss 0.0286\n",
      "Epoch [77/80], Step [500/500], Loss 0.0681\n",
      "Epoch [78/80], Step [100/500], Loss 0.0485\n",
      "Epoch [78/80], Step [200/500], Loss 0.0628\n",
      "Epoch [78/80], Step [300/500], Loss 0.0243\n",
      "Epoch [78/80], Step [400/500], Loss 0.0280\n",
      "Epoch [78/80], Step [500/500], Loss 0.0632\n",
      "Epoch [79/80], Step [100/500], Loss 0.0194\n",
      "Epoch [79/80], Step [200/500], Loss 0.0542\n",
      "Epoch [79/80], Step [300/500], Loss 0.0227\n",
      "Epoch [79/80], Step [400/500], Loss 0.0202\n",
      "Epoch [79/80], Step [500/500], Loss 0.1217\n",
      "Epoch [80/80], Step [100/500], Loss 0.0725\n",
      "Epoch [80/80], Step [200/500], Loss 0.0194\n",
      "Epoch [80/80], Step [300/500], Loss 0.0292\n",
      "Epoch [80/80], Step [400/500], Loss 0.0114\n",
      "Epoch [80/80], Step [500/500], Loss 0.0221\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "total_step = len(train_loader)\n",
    "curr_lr = learning_rate\n",
    "for epoch in range(num_epochs):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optim\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item())) \n",
    "        \n",
    "    # Decay learning rate\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        curr_lr /= 3\n",
    "        update_lr(optimizer, curr_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dc43a9-8c4f-4e9a-8f61-6a67e17095b0",
   "metadata": {},
   "source": [
    "## Test Model with .eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cd65fca-6266-4d9d-be4e-beacec3edf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the test images: 86.1 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print ('Test Accuracy of the model on the test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd3eb4b-5138-4e7b-b4f9-a7c2af0c8e76",
   "metadata": {},
   "source": [
    "## Save the Model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3f177e6-afc3-43a8-b12b-9670fda613ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'AlexNet(CIFAR10).ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f91aab4-c774-45ad-9fef-8dee4a57540d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
